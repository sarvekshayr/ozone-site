
---
sidebar_label: Appendix
---   
 
# Configuration Key Appendix
This page provides a comprehensive overview of all configuration keys available in Ozone.

## Configuration Keys

| **Name** | **Value** | **Tags** | **Description** |
|-|-|-|-|
| `hadoop.hdds.db.rocksdb.WAL_size_limit_MB` | 0MB | `OM`<br/>`SCM`<br/>`DATANODE` | The total size limit of WAL log files. Once the total log file size exceeds this limit, the earliest files will be deleted.Default 0 means no limit. |
| `hadoop.hdds.db.rocksdb.WAL_ttl_seconds` | 1200 | `OM`<br/>`SCM`<br/>`DATANODE` | The lifetime of WAL log files. Default 1200 seconds. |
| `hadoop.hdds.db.rocksdb.logging.enabled` | false | `OM`<br/>`SCM`<br/>`DATANODE` | Enable/Disable RocksDB logging for OM. |
| `hadoop.hdds.db.rocksdb.logging.level` | INFO | `OM`<br/>`SCM`<br/>`DATANODE` | OM RocksDB logging level (INFO/DEBUG/WARN/ERROR/FATAL) |
| `hadoop.hdds.db.rocksdb.writeoption.sync` | false | `OM`<br/>`SCM`<br/>`DATANODE` | Enable/Disable Sync option. If true write will be considered complete, once flushed to persistent storage. If false, writes are flushed asynchronously. |
| `hdds.container.balancer.balancing.iteration.interval` | 70m | `BALANCER` | The interval period between each iteration of Container Balancer. |
| `hdds.container.balancer.datanodes.involved.max.percentage.per.iteration` | 20 | `BALANCER` | Maximum percentage of healthy, in service datanodes that can be involved in balancing in one iteration. |
| `hdds.container.balancer.exclude.containers` |  | `BALANCER` | List of container IDs to exclude from balancing. For example "1, 4, 5" or "1,4,5". |
| `hdds.container.balancer.exclude.datanodes` |  | `BALANCER` | A list of Datanode hostnames or ip addresses separated by commas. The Datanodes specified in this list are excluded from balancing. This configuration is empty by default. |
| `hdds.container.balancer.include.datanodes` |  | `BALANCER` | A list of Datanode hostnames or ip addresses separated by commas. Only the Datanodes specified in this list are balanced. This configuration is empty by default and is applicable only if it is non-empty. |
| `hdds.container.balancer.iterations` | 10 | `BALANCER` | The number of iterations that Container Balancer will run for. |
| `hdds.container.balancer.move.networkTopology.enable` | false | `BALANCER` | whether to take network topology into account when selecting a target for a source. This configuration is false by default. |
| `hdds.container.balancer.move.replication.timeout` | 50m | `BALANCER` | The amount of time to allow a single container's replication from source to target as part of container move. For example, if "hdds.container.balancer.move.timeout" is 65 minutes, then out of those 65 minutes 50 minutes will be the deadline for replication to complete. |
| `hdds.container.balancer.move.timeout` | 65m | `BALANCER` | The amount of time to allow a single container to move from source to target. |
| `hdds.container.balancer.size.entering.target.max` | 26GB | `BALANCER` | The maximum size that can enter a target datanode in each iteration while balancing. This is the sum of data from multiple sources. The value must be greater than the configured (or default) ozone.scm.container.size. |
| `hdds.container.balancer.size.leaving.source.max` | 26GB | `BALANCER` | The maximum size that can leave a source datanode in each iteration while balancing. This is the sum of data moving to multiple targets. The value must be greater than the configured (or default) ozone.scm.container.size. |
| `hdds.container.balancer.size.moved.max.per.iteration` | 500GB | `BALANCER` | The maximum size of data in bytes that will be moved by Container Balancer in one iteration. |
| `hdds.container.balancer.trigger.du.before.move.enable` | false | `BALANCER` | whether to send command to all the healthy and in-service data nodes to run du immediately before startinga balance iteration. note that running du is very time consuming , especially when the disk usage rate of a data node is very high |
| `hdds.container.balancer.utilization.threshold` | 10 | `BALANCER` | Threshold is a percentage in the range of 0 to 100. A cluster is considered balanced if for each datanode, the utilization of the datanode (used space to capacity ratio) differs from the utilization of the cluster (used space to capacity ratio of the entire cluster) no more than the threshold. |
| `hdds.container.scrub.data.scan.interval` | 7d | `STORAGE` | Minimum time interval between two iterations of container data scanning. If an iteration takes less time than this, the scanner will wait before starting the next iteration. Unit could be defined with postfix (ns,ms,s,m,h,d). |
| `hdds.container.scrub.dev.data.scan.enabled` | true | `STORAGE` | Can be used to disable the background container data scanner for developer testing purposes. |
| `hdds.container.scrub.dev.metadata.scan.enabled` | true | `STORAGE` | Can be used to disable the background container metadata scanner for developer testing purposes. |
| `hdds.container.scrub.enabled` | true | `STORAGE` | Config parameter to enable all container scanners. |
| `hdds.container.scrub.metadata.scan.interval` | 3h | `STORAGE` | Config parameter define time interval between two metadata scans by container scanner. Unit could be defined with postfix (ns,ms,s,m,h,d). |
| `hdds.container.scrub.min.gap` | 15m | `DATANODE` | The minimum gap between two successive scans of the same container. Unit could be defined with postfix (ns,ms,s,m,h,d). |
| `hdds.container.scrub.on.demand.volume.bytes.per.second` | 5242880 | `STORAGE` | Config parameter to throttle I/O bandwidth used by the demand container scanner per volume. |
| `hdds.container.scrub.volume.bytes.per.second` | 5242880 | `STORAGE` | Config parameter to throttle I/O bandwidth used by scanner per volume. |
| `hdds.datanode.block.delete.command.worker.interval` | 2s | `DATANODE` | The interval between DeleteCmdWorker execution of delete commands. |
| `hdds.datanode.block.delete.max.lock.wait.timeout` | 100ms | `DATANODE`<br/>`DELETION` | Timeout for the thread used to process the delete block command to wait for the container lock. |
| `hdds.datanode.block.delete.queue.limit` | 5 | `DATANODE` | The maximum number of block delete commands queued on a datanode, This configuration is also used by the SCM to control whether to send delete commands to the DN. If the DN has more commands waiting in the queue than this value, the SCM will not send any new block delete commands. until the DN has processed some commands and the queue length is reduced. |
| `hdds.datanode.block.delete.threads.max` | 5 | `DATANODE` | The maximum number of threads used to handle delete blocks on a datanode |
| `hdds.datanode.block.deleting.limit.per.interval` | 5000 | `SCM`<br/>`DELETION` | Number of blocks to be deleted in an interval. |
| `hdds.datanode.block.deleting.max.lock.holding.time` | 1s | `DATANODE`<br/>`DELETION` | This configuration controls the maximum time that the block deleting service can hold the lock during the deletion of blocks. Once this configured time period is reached, the service will release and re-acquire the lock. This is not a hard limit as the time check only occurs after the completion of each transaction, which means the actual execution time may exceed this limit. Unit could be defined with postfix (ns,ms,s,m,h,d). |
| `hdds.datanode.block.deleting.service.interval` | 60s | `SCM`<br/>`DELETION` | Time interval of the Datanode block deleting service. The block deleting service runs on Datanode periodically and deletes blocks queued for deletion. Unit could be defined with postfix (ns,ms,s,m,h,d). |
| `hdds.datanode.check.empty.container.dir.on.delete` | false | `DATANODE` | Boolean Flag to decide whether to check container directory or not to determine container is empty |
| `hdds.datanode.chunk.data.validation.check` | false | `DATANODE` | Enable safety checks such as checksum validation for Ratis calls. |
| `hdds.datanode.command.queue.limit` | 5000 | `DATANODE` | The default maximum number of commands in the queue and command type's sub-queue on a datanode |
| `hdds.datanode.container.close.threads.max` | 3 | `DATANODE` | The maximum number of threads used to close containers on a datanode |
| `hdds.datanode.container.delete.threads.max` | 2 | `DATANODE` | The maximum number of threads used to delete containers on a datanode |
| `hdds.datanode.container.schema.v3.enabled` | true | `DATANODE` | Enable use of container schema v3(one rocksdb per disk). |
| `hdds.datanode.container.schema.v3.key.separator` | | | `DATANODE` | The default separator between Container ID and container meta key name. |
| `hdds.datanode.df.refresh.period` | 5m | `DATANODE` | Disk space usage information will be refreshed with thespecified period following the completion of the last check. |
| `hdds.datanode.disk.check.io.failures.tolerated` | 1 | `DATANODE` | The number of IO tests out of the last `hdds.datanode.disk.check.io.test.count` test run that are allowed to fail before the volume is marked as failed. |
| `hdds.datanode.disk.check.io.file.size` | 100B | `DATANODE` | The size of the temporary file that will be synced to the disk and read back to assess its health. The contents of the file will be stored in memory during the duration of the check. |
| `hdds.datanode.disk.check.io.test.count` | 3 | `DATANODE` | The number of IO tests required to determine if a disk has failed. Each disk check does one IO test. The volume will be failed if more than hdds.datanode.disk.check.io.failures.tolerated out of the last hdds.datanode.disk.check.io.test.count runs failed. Set to 0 to disable disk IO checks. |
| `hdds.datanode.disk.check.min.gap` | 10m | `DATANODE` | The minimum gap between two successive checks of the same Datanode volume. Unit could be defined with postfix (ns,ms,s,m,h,d). |
| `hdds.datanode.disk.check.timeout` | 10m | `DATANODE` | Maximum allowed time for a disk check to complete. If the check does not complete within this time interval then the disk is declared as failed. Unit could be defined with postfix (ns,ms,s,m,h,d). |
| `hdds.datanode.du.factory.classname` |  | `DATANODE` | The fully qualified name of the factory class that creates objects for providing disk space usage information. It should implement the SpaceUsageCheckFactory interface. |
| `hdds.datanode.du.refresh.period` | 1h | `DATANODE` | Disk space usage information will be refreshed with thespecified period following the completion of the last check. |
| `hdds.datanode.failed.data.volumes.tolerated` | -1 | `DATANODE` | The number of data volumes that are allowed to fail before a datanode stops offering service. Config this to -1 means unlimited, but we should have at least one good volume left. |
| `hdds.datanode.failed.db.volumes.tolerated` | -1 | `DATANODE` | The number of db volumes that are allowed to fail before a datanode stops offering service. Config this to -1 means unlimited, but we should have at least one good volume left. |
| `hdds.datanode.failed.metadata.volumes.tolerated` | -1 | `DATANODE` | The number of metadata volumes that are allowed to fail before a datanode stops offering service. Config this to -1 means unlimited, but we should have at least one good volume left. |
| `hdds.datanode.periodic.disk.check.interval.minutes` | 60 | `DATANODE` | Periodic disk check run interval in minutes. |
| `hdds.datanode.read.chunk.threads.per.volume` | 10 | `DATANODE` | Number of threads per volume that Datanode will use for reading replicated chunks. |
| `hdds.datanode.recovering.container.scrubbing.service.interval` | 1m | `SCM`<br/>`DELETION` | Time interval of the stale recovering container scrubbing service. The recovering container scrubbing service runs on Datanode periodically and deletes stale recovering container Unit could be defined with postfix (ns,ms,s,m,h,d). |
| `hdds.datanode.replication.outofservice.limit.factor` | 2.0 | `DATANODE`<br/>`SCM` | Decommissioning and maintenance nodes can handle morereplication commands than in-service nodes due to reduced load. This multiplier determines the increased queue capacity and executor pool size. |
| `hdds.datanode.replication.port` | 9886 | `DATANODE`<br/>`MANAGEMENT` | Port used for the server2server replication server |
| `hdds.datanode.replication.queue.limit` | 4096 | `DATANODE` | The maximum number of queued requests for container replication |
| `hdds.datanode.replication.streams.limit` | 10 | `DATANODE` | The maximum number of replication commands a single datanode can execute simultaneously |
| `hdds.datanode.replication.zerocopy.enabled` | true | `DATANODE`<br/>`SCM` | Specify if zero-copy should be enabled for replication protocol. |
| `hdds.datanode.rocksdb.auto-compaction-small-sst-file` | true | `DATANODE` | Auto compact small SST files (rocksdb.auto-compaction-small-sst-file-size-threshold) when count exceeds (rocksdb.auto-compaction-small-sst-file-num-threshold) |
| `hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold` | 512 | `DATANODE` | Auto compaction will happen if the number of small SST files exceeds this threshold. |
| `hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold` | 1MB | `DATANODE` | SST files smaller than this configuration will be auto compacted. |
| `hdds.datanode.rocksdb.delete-obsolete-files-period` | 1h | `DATANODE` | Periodicity when obsolete files get deleted. Default is 1h. |
| `hdds.datanode.rocksdb.log.level` | INFO | `DATANODE` | The user log level of RocksDB(DEBUG/INFO/WARN/ERROR/FATAL)) |
| `hdds.datanode.rocksdb.log.max-file-num` | 64 | `DATANODE` | The max user log file number to keep for each RocksDB |
| `hdds.datanode.rocksdb.log.max-file-size` | 32MB | `DATANODE` | The max size of each user log file of RocksDB. O means no size limit. |
| `hdds.datanode.rocksdb.max-open-files` | 1024 | `DATANODE` | The total number of files that a RocksDB can open. |
| `hdds.datanode.wait.on.all.followers` | false | `DATANODE` | Defines whether the leader datanode will wait for bothfollowers to catch up before removing the stateMachineData from the cache. |
| `hdds.prometheus.endpoint.token` |  | `SECURITY`<br/>`MANAGEMENT` | Allowed authorization token while using prometheus servlet endpoint. This will disable SPNEGO based authentication on the endpoint. |
| `hdds.ratis.client.exponential.backoff.base.sleep` | 4s | `OZONE`<br/>`CLIENT`<br/>`PERFORMANCE` | Specifies base sleep for exponential backoff retry policy. With the default base sleep of 4s, the sleep duration for ith retry is min(4 * pow(2, i), max_sleep) * r, where r is random number in the range [0.5, 1.5). |
| `hdds.ratis.client.exponential.backoff.max.sleep` | 40s | `OZONE`<br/>`CLIENT`<br/>`PERFORMANCE` | The sleep duration obtained from exponential backoff policy is limited by the configured max sleep. Refer dfs.ratis.client.exponential.backoff.base.sleep for further details. |
| `hdds.ratis.client.multilinear.random.retry.policy` | 5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10 | `OZONE`<br/>`CLIENT`<br/>`PERFORMANCE` | Specifies multilinear random retry policy to be used by ratis client. e.g. given pairs of number of retries and sleep time (n0, t0), (n1, t1), ..., for the first n0 retries sleep duration is t0 on average, the following n1 retries sleep duration is t1 on average, and so on. |
| `hdds.ratis.client.request.watch.timeout` | 3m | `OZONE`<br/>`CLIENT`<br/>`PERFORMANCE` | Timeout for ratis client watch request. |
| `hdds.ratis.client.request.watch.type` | ALL_COMMITTED | `OZONE`<br/>`CLIENT`<br/>`PERFORMANCE` | Desired replication level when Ozone client's Raft client calls watch(), ALL_COMMITTED or MAJORITY_COMMITTED. MAJORITY_COMMITTED increases write performance by reducing watch() latency when an Ozone datanode is slow in a pipeline, at the cost of potential read latency increasing due to read retries to different datanodes. |
| `hdds.ratis.client.request.write.timeout` | 5m | `OZONE`<br/>`CLIENT`<br/>`PERFORMANCE` | Timeout for ratis client write request. |
| `hdds.ratis.client.retry.policy` | org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator | `OZONE`<br/>`CLIENT`<br/>`PERFORMANCE` | The class name of the policy for retry. |
| `hdds.ratis.client.retrylimited.max.retries` | 180 | `OZONE`<br/>`CLIENT`<br/>`PERFORMANCE` | Number of retries for ratis client request. |
| `hdds.ratis.client.retrylimited.retry.interval` | 1s | `OZONE`<br/>`CLIENT`<br/>`PERFORMANCE` | Interval between successive retries for a ratis client request. |
| `hdds.ratis.raft.client.async.outstanding-requests.max` | 32 | `OZONE`<br/>`CLIENT`<br/>`PERFORMANCE` | Controls the maximum number of outstanding async requests that can be handled by the Standalone as well as Ratis client. |
| `hdds.ratis.raft.client.rpc.request.timeout` | 60s | `OZONE`<br/>`CLIENT`<br/>`PERFORMANCE` | The timeout duration for ratis client request (except for watch request). It should be set greater than leader election timeout in Ratis. |
| `hdds.ratis.raft.client.rpc.watch.request.timeout` | 180s | `OZONE`<br/>`CLIENT`<br/>`PERFORMANCE` | The timeout duration for ratis client watch request. Timeout for the watch API in Ratis client to acknowledge a particular request getting replayed to all servers. It is highly recommended for the timeout duration to be strictly longer than Ratis server watch timeout `(hdds.ratis.raft.server.watch.timeout)` |
| `hdds.ratis.raft.grpc.flow.control.window` | 5MB | `OZONE`<br/>`CLIENT`<br/>`PERFORMANCE` | This parameter tells how much data grpc client can send to grpc server with out receiving any ack(WINDOW_UPDATE) packet from server. This parameter should be set in accordance with chunk size. Example: If Chunk size is 4MB, considering some header size in to consideration, this can be set 5MB or greater. Tune this parameter accordingly, as when it is set with a value lesser than chunk size it degrades the ozone client performance. |
| `hdds.ratis.raft.grpc.message.size.max` | 32MB | `OZONE`<br/>`CLIENT`<br/>`PERFORMANCE` | Maximum message size allowed to be received by Grpc Channel (Server). |
| `hdds.ratis.raft.server.datastream.client.pool.size` | 10 | `OZONE`<br/>`DATANODE`<br/>`RATIS`<br/>`DATASTREAM` | Maximum number of client proxy in NettyServerStreamRpc for datastream write. |
| `hdds.ratis.raft.server.datastream.request.threads` | 20 | `OZONE`<br/>`DATANODE`<br/>`RATIS`<br/>`DATASTREAM` | Maximum number of threads in the thread pool for datastream request. |
| `hdds.ratis.raft.server.delete.ratis.log.directory` | true | `OZONE`<br/>`DATANODE`<br/>`RATIS` | Flag to indicate whether ratis log directory will becleaned up during pipeline remove. |
| `hdds.ratis.raft.server.leaderelection.pre-vote` | true | `OZONE`<br/>`DATANODE`<br/>`RATIS` | Flag to enable/disable ratis election pre-vote. |
| `hdds.ratis.raft.server.log.appender.wait-time.min` | 0us | `OZONE`<br/>`DATANODE`<br/>`RATIS`<br/>`PERFORMANCE` | The minimum wait time between two appendEntries calls. In some error conditions, the leader may keep retrying appendEntries. If it happens, increasing this value to, say, 5us (microseconds) can help avoid the leader being too busy retrying. |
| `hdds.ratis.raft.server.notification.no-leader.timeout` | 300s | `OZONE`<br/>`DATANODE`<br/>`RATIS` | Time out duration after which StateMachine gets notified that leader has not been elected for a long time and leader changes its role to Candidate. |
| `hdds.ratis.raft.server.rpc.request.timeout` | 60s | `OZONE`<br/>`DATANODE`<br/>`RATIS` | The timeout duration of the ratis write request on Ratis Server. |
| `hdds.ratis.raft.server.rpc.slowness.timeout` | 300s | `OZONE`<br/>`DATANODE`<br/>`RATIS` | Timeout duration after which stateMachine will be notified that follower is slow. StateMachine will close down the pipeline. |
| `hdds.ratis.raft.server.watch.timeout` | 30s | `OZONE`<br/>`DATANODE`<br/>`RATIS` | The timeout duration for watch request on Ratis Server. Timeout for the watch request in Ratis server to acknowledge a particular request is replayed to all servers. It is highly recommended for the timeout duration to be strictly shorter than Ratis client watch timeout (hdds.ratis.raft.client.rpc.watch.request.timeout). |
| `hdds.ratis.raft.server.write.element-limit` | 1024 | `OZONE`<br/>`DATANODE`<br/>`RATIS`<br/>`PERFORMANCE` | Maximum number of pending requests after which the leader starts rejecting requests from client. |
| `hdds.ratis.server.num.snapshots.retained` | 5 | `STORAGE` | Config parameter to specify number of old snapshots retained at the Ratis leader. |
| `hdds.scm.block.deleting.service.interval` | 60s | `SCM`<br/>`DELETION` | Time interval of the scm block deleting service. The block deletingservice runs on SCM periodically and deletes blocks queued for deletion. Unit could be defined with postfix (ns,ms,s,m,h,d). |
| `hdds.scm.block.deletion.per-interval.max` | 100000 | `SCM`<br/>`DELETION` | Maximum number of blocks which SCM processes during an interval. The block num is counted at the replica level.If SCM has 100000 blocks which need to be deleted and the configuration is 5000 then it would only send 5000 blocks for deletion to the datanodes. |
| `hdds.scm.ec.pipeline.choose.policy.impl` | org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy | `SCM`<br/>`PIPELINE` | Sets the policy for choosing an EC pipeline. The value should be the full name of a class which implements org.apache.hadoop.hdds.scm.PipelineChoosePolicy. The class decides which pipeline will be used when selecting an EC Pipeline. If not set, org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy will be used as default value. One of the following values can be used: (1) org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy : chooses a pipeline randomly. (2) org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.HealthyPipelineChoosePolicy : chooses a healthy pipeline randomly. (3) org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.CapacityPipelineChoosePolicy : chooses the pipeline with lower utilization from two random pipelines. Note that random choose method will be executed twice in this policy.(4) org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RoundRobinPipelineChoosePolicy : chooses a pipeline in a round robin fashion. Intended for troubleshooting and testing purposes only. |
| `hdds.scm.http.auth.kerberos.keytab` |  | `SECURITY` | The keytab file used by SCM http server to login as its service principal. |
| `hdds.scm.http.auth.kerberos.principal` |  | `SECURITY` | This Kerberos principal is used when communicating to the HTTP server of SCM.The protocol used is SPNEGO. |
| `hdds.scm.init.default.layout.version` | -1 | `SCM`<br/>`UPGRADE` | Default Layout Version to init the SCM with. Intended to be used in tests to finalize from an older version of SCM to the latest. By default, SCM init uses the highest layout version. |
| `hdds.scm.kerberos.keytab.file` |  | `SECURITY`<br/>`OZONE` | The keytab file used by SCM daemon to login as its service principal. |
| `hdds.scm.kerberos.principal` |  | `SECURITY`<br/>`OZONE` | This Kerberos principal is used by the SCM service. |
| `hdds.scm.pipeline.choose.policy.impl` | org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy | `SCM`<br/>`PIPELINE` | Sets the policy for choosing a pipeline for a Ratis container. The value should be the full name of a class which implements org.apache.hadoop.hdds.scm.PipelineChoosePolicy. The class decides which pipeline will be used to find or allocate Ratis containers. If not set, org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy will be used as default value. One of the following values can be used: (1) org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy : chooses a pipeline randomly. (2) org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.HealthyPipelineChoosePolicy : chooses a healthy pipeline randomly. (3) org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.CapacityPipelineChoosePolicy : chooses the pipeline with lower utilization from two random pipelines. Note that random choose method will be executed twice in this policy.(4) org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RoundRobinPipelineChoosePolicy : chooses a pipeline in a round robin fashion. Intended for troubleshooting and testing purposes only. |
| `hdds.scm.replication.container.inflight.deletion.limit` | 0 | `SCM`<br/>`OZONE` | This property is used to limit the maximum number of inflight deletion. |
| `hdds.scm.replication.container.inflight.replication.limit` | 0 | `SCM`<br/>`OZONE` | This property is used to limit the maximum number of inflight replication. |
| `hdds.scm.replication.datanode.delete.container.limit` | 40 | `SCM`<br/>`DATANODE` | A limit to restrict the total number of delete container commands queued on a datanode. Note this is intended to be a temporary config until we have a more dynamic way of limiting load |
| `hdds.scm.replication.datanode.reconstruction.weight` | 3 | `SCM`<br/>`DATANODE` | When counting the number of replication commands on a datanode, the number of reconstruction commands is multiplied by this weight to ensure reconstruction commands use more of the capacity, as they are more expensive to process. |
| `hdds.scm.replication.datanode.replication.limit` | 20 | `SCM`<br/>`DATANODE` | A limit to restrict the total number of replication and reconstruction commands queued on a datanode. Note this is intended to be a temporary config until we have a more dynamic way of limiting load. |
| `hdds.scm.replication.enable.legacy` | false | `SCM`<br/>`OZONE` | If true, LegacyReplicationManager will handle RATIS containers while ReplicationManager will handle EC containers. If false, ReplicationManager will handle both RATIS and EC. |
| `hdds.scm.replication.event.timeout` | 10m | `SCM`<br/>`OZONE` | Timeout for the container replication/deletion commands sent to datanodes. After this timeout the command will be retried. |
| `hdds.scm.replication.event.timeout.datanode.offset` | 30s | `SCM`<br/>`OZONE` | The amount of time to subtract from `hdds.scm.replication.event.timeout` to give a deadline on the datanodes which is less than the SCM timeout. This ensures the datanodes will not process a command after SCM believes it should have expired. |
| `hdds.scm.replication.inflight.limit.factor` | 0.75 | `SCM` | The overall replication task limit on a cluster is the number healthy nodes, times the datanode.replication.limit. This factor, which should be between zero and 1, scales that limit down to reduce the overall number of replicas pending creation on the cluster. A setting of zero disables global limit checking. A setting of 1 effectively disables it, by making the limit equal to the above equation. However if there are many decommissioning nodes on the cluster, the decommission nodes will have a higher than normal limit, so the setting of 1 may still provide some limit in extreme circumstances. |
| `hdds.scm.replication.maintenance.remaining.redundancy` | 1 | `SCM`<br/>`OZONE` | The number of redundant containers in a group which must be available for a node to enter maintenance. If putting a node into maintenance reduces the redundancy below this value , the node will remain in the ENTERING_MAINTENANCE state until a new replica is created. For Ratis containers, the default value of 1 ensures at least two replicas are online, meaning 1 more can be lost without data becoming unavailable. For any EC container it will have at least dataNum + 1 online, allowing the loss of 1 more replica before data becomes unavailable. Currently only EC containers use this setting. Ratis containers use `hdds.scm.replication.maintenance.replica.minimum.` For EC, if nodes are in maintenance, it is likely reconstruction reads will be required if some of the data replicas are offline. This is seamless to the client, but will affect read performance. |
| `hdds.scm.replication.maintenance.replica.minimum` | 2 | `SCM`<br/>`OZONE` | The minimum number of container replicas which must be available for a node to enter maintenance. If putting a node into maintenance reduces the available replicas for any container below this level, the node will remain in the entering maintenance state until a new replica is created. |
| `hdds.scm.replication.over.replicated.interval` | 30s | `SCM`<br/>`OZONE` | How frequently to check if there are work to process on the over replicated queue |
| `hdds.scm.replication.push` | true | `SCM`<br/>`DATANODE` | If false, replication happens by asking the target to pull from source nodes. If true, the source node is asked to push to the target node. |
| `hdds.scm.replication.thread.interval` | 300s | `SCM`<br/>`OZONE` | There is a replication monitor thread running inside SCM which takes care of replicating the containers in the cluster. This property is used to configure the interval in which that thread runs. |
| `hdds.scm.replication.under.replicated.interval` | 30s | `SCM`<br/>`OZONE` | How frequently to check if there are work to process on the under replicated queue |
| `hdds.scm.unknown-container.action` | WARN | `SCM`<br/>`MANAGEMENT` | The action taken by SCM to process unknown containers that reported by Datanodes. The default action is just logging container not found warning, another available action is DELETE action. These unknown containers will be deleted under this action way. |
| `hdds.scmclient.failover.max.retry` | 15 | `OZONE`<br/>`SCM`<br/>`CLIENT` | Max retry count for SCM Client when failover happens. |
| `hdds.scmclient.failover.retry.interval` | 2s | `OZONE`<br/>`SCM`<br/>`CLIENT` | SCM Client timeout on waiting for the next connection retry to other SCM IP. The default value is set to 2 seconds. |
| `hdds.scmclient.max.retry.timeout` | 10m | `OZONE`<br/>`SCM`<br/>`CLIENT` | Max retry timeout for SCM Client |
| `hdds.scmclient.rpc.timeout` | 15m | `OZONE`<br/>`SCM`<br/>`CLIENT` | RpcClient timeout on waiting for the response from SCM. The default value is set to 15 minutes. If ipc.client.ping is set to true and this rpc-timeout is greater than the value of ipc.ping.interval, the effective value of the rpc-timeout is rounded up to multiple of ipc.ping.interval. |
| `ozone.client.bytes.per.checksum` | 16KB | `CLIENT`<br/>`CRYPTO_COMPLIANCE` | Checksum will be computed for every bytes per checksum number of bytes and stored sequentially. The minimum value for this config is 8KB. |
| `ozone.client.checksum.combine.mode` | COMPOSITE_CRC | `CLIENT` | The combined checksum type [MD5MD5CRC / COMPOSITE_CRC] determines which algorithm would be used to compute file checksum.COMPOSITE_CRC calculates the combined CRC of the whole file, where the lower-level chunk/block checksums are combined into file-level checksum.MD5MD5CRC calculates the MD5 of MD5 of checksums of individual chunks.Default checksum type is COMPOSITE_CRC. |
| `ozone.client.checksum.type` | CRC32 | `CLIENT`<br/>`CRYPTO_COMPLIANCE` | The checksum type [NONE/ CRC32/ CRC32C/ SHA256/ MD5] determines which algorithm would be used to compute checksum for chunk data. Default checksum type is CRC32. |
| `ozone.client.datastream.buffer.flush.size` | 16MB | `CLIENT` | The boundary at which putBlock is executed |
| `ozone.client.datastream.min.packet.size` | 1MB | `CLIENT` | The maximum size of the ByteBuffer (used via ratis streaming) |
| `ozone.client.datastream.pipeline.mode` | true | `CLIENT` | Streaming write support both pipeline mode(datanode1->datanode2->datanode3) and star mode(datanode1->datanode2, datanode1->datanode3). By default we use pipeline mode. |
| `ozone.client.datastream.window.size` | 64MB | `CLIENT` | Maximum size of BufferList(used for retry) size per BlockDataStreamOutput instance |
| `ozone.client.ec.reconstruct.stripe.read.pool.limit` | 30 | `CLIENT` | Thread pool max size for parallelly read available ec chunks to reconstruct the whole stripe. |
| `ozone.client.ec.reconstruct.stripe.write.pool.limit` | 30 | `CLIENT` | Thread pool max size for parallelly write available ec chunks to reconstruct the whole stripe. |
| `ozone.client.ec.stripe.queue.size` | 2 | `CLIENT` | The max number of EC stripes can be buffered in client before flushing into datanodes. |
| `ozone.client.exclude.nodes.expiry.time` | 600000 | `CLIENT` | Time after which an excluded node is reconsidered for writes. If the value is zero, the node is excluded for the life of the client |
| `ozone.client.fs.default.bucket.layout` | FILE_SYSTEM_OPTIMIZED | `CLIENT` | The bucket layout used by buckets created using OFS. Valid values include FILE_SYSTEM_OPTIMIZED and LEGACY |
| `ozone.client.max.ec.stripe.write.retries` | 10 | `CLIENT` | Ozone EC client to retry stripe to new block group on failures. |
| `ozone.client.max.retries` | 5 | `CLIENT` | Maximum number of retries by Ozone Client on encountering exception while writing a key |
| `ozone.client.read.max.retries` | 3 | `CLIENT` | Maximum number of retries by Ozone Client on encountering connectivity exception when reading a key. |
| `ozone.client.read.retry.interval` | 1 | `CLIENT` | Indicates the time duration in seconds a client will wait before retrying a read key request on encountering a connectivity excepetion from Datanodes . By default the interval is 1 second |
| `ozone.client.retry.interval` | 0 | `CLIENT` | Indicates the time duration a client will wait before retrying a write key request on encountering an exception. By default there is no wait |
| `ozone.client.stream.buffer.flush.delay` | true | `CLIENT` | Default true, when call flush() and determine whether the data in the current buffer is greater than `ozone.client.stream.buffer.size,` if greater than then send buffer to the datanode. You can turn this off by setting this configuration to false. |
| `ozone.client.stream.buffer.flush.size` | 16MB | `CLIENT` | Size which determines at what buffer position a partial flush will be initiated during write. It should be a multiple of `ozone.client.stream.buffer.size` |
| `ozone.client.stream.buffer.increment` | 0B | `CLIENT` | Buffer (defined by `ozone.client.stream.buffer.size)` will be incremented with this steps. If zero, the full buffer will be created at once. Setting it to a variable between 0 and `ozone.client.stream.buffer.size` can reduce the memory usage for very small keys, but has a performance overhead. |
| `ozone.client.stream.buffer.max.size` | 32MB | `CLIENT` | Size which determines at what buffer position write call be blocked till acknowledgement of the first partial flush happens by all servers. |
| `ozone.client.stream.buffer.size` | 4MB | `CLIENT` | The size of chunks the client will send to the server |
| `ozone.client.verify.checksum` | true | `CLIENT` | Ozone client to verify checksum of the checksum blocksize data. |
| `ozone.csi.default-volume-size` | 1000000000 | `STORAGE` | The default size of the create volumes (if not specified). |
| `ozone.csi.mount.command` | goofys --endpoint %s %s %s | `STORAGE` | This is the mount command which is used to publish volume. these %s will be replicated by s3gAddress, volumeId and target path. |
| `ozone.csi.owner` |  | `STORAGE` | This is the username which is used to create the requested storage. Used as a hadoop username and the generated ozone volume used to store all the buckets. WARNING: It can be a security hole to use CSI in a secure environments as ALL the users can request the mount of a specific bucket via the CSI interface. |
| `ozone.csi.s3g.address` | http://localhost:9878 | `STORAGE` | The address of S3 Gateway endpoint. |
| `ozone.csi.socket` | /var/lib/csi.sock | `STORAGE` | The socket where all the CSI services will listen (file name). |
| `ozone.om.client.rpc.timeout` | 15m | `OZONE`<br/>`OM`<br/>`CLIENT` | RpcClient timeout on waiting for the response from OzoneManager. The default value is set to 15 minutes. If ipc.client.ping is set to true and this rpc-timeout is greater than the value of ipc.ping.interval, the effective value of the rpc-timeout is rounded up to multiple of ipc.ping.interval. |
| `ozone.om.client.trash.core.pool.size` | 5 | `OZONE`<br/>`OM`<br/>`CLIENT` | Total number of threads in pool for the Trash Emptier |
| `ozone.om.group.rights` | ALL | `OM`<br/>`SECURITY` | Default group permissions set for an object in OzoneManager. |
| `ozone.om.grpc.port` | 8981 | `MANAGEMENT` | Port used for the GrpcOmTransport OzoneManagerServiceGrpc server |
| `ozone.om.ha.raft.server.log.appender.wait-time.min` | 0ms | `OZONE`<br/>`OM`<br/>`RATIS`<br/>`PERFORMANCE` | Minimum wait time between two appendEntries calls. |
| `ozone.om.ha.raft.server.retrycache.expirytime` | 300s | `OZONE`<br/>`OM`<br/>`RATIS` | The timeout duration of the retry cache. |
| `ozone.om.init.default.layout.version` | -1 | `OM`<br/>`UPGRADE` | Default Layout Version to init the OM with. Intended to be used in tests to finalize from an older version of OM to the latest. By default, OM init uses the highest layout version. |
| `ozone.om.upgrade.finalization.ratis.based.timeout` | 30s | `OM`<br/>`UPGRADE` | Maximum time to wait for a slow follower to be finalized through a Ratis snapshot. This is an advanced config, and needs to be changed only under a special circumstance when the leader OM has purged the finalize request from its logs, and a follower OM was down during upgrade finalization. Default is 30s. |
| `ozone.om.user.rights` | ALL | `OM`<br/>`SECURITY` | Default user permissions set for an object in OzoneManager. |
| `ozone.recon.kerberos.keytab.file` |  | `SECURITY`<br/>`RECON`<br/>`OZONE` | The keytab file used by Recon daemon to login as its service principal. |
| `ozone.recon.kerberos.principal` |  | `SECURITY`<br/>`RECON`<br/>`OZONE` | This Kerberos principal is used by the Recon service. |
| `ozone.recon.security.client.datanode.container.protocol.acl` | * | `SECURITY`<br/>`RECON`<br/>`OZONE` | Comma separated acls (users, groups) allowing clients accessing datanode container protocol |
| `ozone.recon.sql.db.auto.commit` | true | `STORAGE`<br/>`RECON`<br/>`OZONE` | Sets the Ozone Recon database connection property of auto-commit to true/false. |
| `ozone.recon.sql.db.conn.idle.max.age` | 3600s | `STORAGE`<br/>`RECON`<br/>`OZONE` | Sets maximum time to live for idle connection in seconds. |
| `ozone.recon.sql.db.conn.idle.test` | SELECT 1 | `STORAGE`<br/>`RECON`<br/>`OZONE` | The query to send to the DB to maintain keep-alives and test for dead connections. |
| `ozone.recon.sql.db.conn.idle.test.period` | 60s | `STORAGE`<br/>`RECON`<br/>`OZONE` | Sets maximum time to live for idle connection in seconds. |
| `ozone.recon.sql.db.conn.max.active` | 5 | `STORAGE`<br/>`RECON`<br/>`OZONE` | The max active connections to the SQL database. |
| `ozone.recon.sql.db.conn.max.age` | 1800s | `STORAGE`<br/>`RECON`<br/>`OZONE` | Sets maximum time a connection can be active in seconds. |
| `ozone.recon.sql.db.conn.timeout` | 30000ms | `STORAGE`<br/>`RECON`<br/>`OZONE` | Sets time in milliseconds before call to getConnection is timed out. |
| `ozone.recon.sql.db.driver` | org.apache.derby.jdbc.EmbeddedDriver | `STORAGE`<br/>`RECON`<br/>`OZONE` | Recon SQL DB driver class. Defaults to Derby. |
| `ozone.recon.sql.db.jdbc.url` | jdbc:derby:${ozone.recon.db.dir}/ozone_recon_derby.db | `STORAGE`<br/>`RECON`<br/>`OZONE` | Ozone Recon SQL database jdbc url. |
| `ozone.recon.sql.db.jooq.dialect` | DERBY | `STORAGE`<br/>`RECON`<br/>`OZONE` | Recon internally uses Jooq to talk to its SQL DB. By default, we support Derby and Sqlite out of the box. Please refer to https://www.jooq.org/javadoc/latest/org.jooq/org/jooq/SQLDialect.html to specify different dialect. |
| `ozone.recon.sql.db.password` |  | `STORAGE`<br/>`RECON`<br/>`OZONE` | Ozone Recon SQL database password. |
| `ozone.recon.sql.db.username` |  | `STORAGE`<br/>`RECON`<br/>`OZONE` | Ozone Recon SQL database username. |
| `ozone.recon.task.containercounttask.interval` | 60s | `RECON`<br/>`OZONE` | The time interval to wait between each runs of container count task. |
| `ozone.recon.task.missingcontainer.interval` | 300s | `RECON`<br/>`OZONE` | The time interval of the periodic check for unhealthy containers in the cluster as reported by Datanodes. |
| `ozone.recon.task.pipelinesync.interval` | 300s | `RECON`<br/>`OZONE` | The time interval of periodic sync of pipeline state from SCM to Recon. |
| `ozone.recon.task.safemode.wait.threshold` | 300s | `RECON`<br/>`OZONE` | The time interval to wait for starting container health task and pipeline sync task before recon exits out of safe or warmup mode. |
| `ozone.replication.allowed-configs` | ^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$ | `STORAGE` | Regular expression to restrict enabled replication schemes |
| `ozone.scm.ec.pipeline.minimum` | 5 | `STORAGE` | The minimum number of pipelines to have open for each Erasure Coding configuration |
| `ozone.scm.ec.pipeline.per.volume.factor` | 1 | `SCM` | TODO |
| `ozone.scm.ha.raft.server.log.appender.wait-time.min` | 0ms | `OZONE`<br/>`SCM`<br/>`RATIS`<br/>`PERFORMANCE` | Minimum wait time between two appendEntries calls. |
| `ozone.service.shutdown.timeout` | 60s | `OZONE`<br/>`OM`<br/>`SCM`<br/>`DATANODE`<br/>`RECON`<br/>`S3GATEWAY` | Timeout to wait for each shutdown operation to completeIf a hook takes longer than this time to complete, it will be interrupted, so the service will shutdown. This allows the service shutdown to recover from a blocked operation. The minimum duration of the timeout is 1 second, if hook has been configured with a timeout less than 1 second. |
| `scm.container.client.idle.threshold` | 10s | `OZONE`<br/>`PERFORMANCE` | In the standalone pipelines, the SCM clients use netty to communicate with the container. It also uses connection pooling to reduce client side overheads. This allows a connection to stay idle for a while before the connection is closed. |
| `scm.container.client.max.size` | 256 | `OZONE`<br/>`PERFORMANCE` | Controls the maximum number of connections that are cached via client connection pooling. If the number of connections exceed this count, then the oldest idle connection is evicted. |

